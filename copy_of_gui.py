# -*- coding: utf-8 -*-
"""Copy of GUI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1coholSy2yuTHmwq4SEYNn5mTXeC8Y8Ou
"""

!pip install gradio # This is to install the Gradio package and make the webpage possible
!pip install transformers # This makes the the ability to transform encoded information possible
!pip install sentence_transformers # This is the same but for sentences
!pip install faiss-cpu  # Use this for CPU only or
!pip install faiss-gpu  # Use this for GPU acceleration (if you have a compatible GPU)

import gradio as gr
import faiss
import pandas as pd
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
from sentence_transformers import SentenceTransformer

# Load dataset
dataset = pd.read_csv("/content/drive/MyDrive/Final/sampled_dataset_no_nulls_only_EN_NEW.csv")

# https://faiss.ai/index.html This link is for FAISS documentation
# Load FAISS index
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.IndexFlatL2(embedding_model.get_sentence_embedding_dimension())
index.add(embedding_model.encode(dataset["summary"].tolist(), convert_to_numpy=True))

# Load your fine-tuned LLM
model_path = "/content/drive/MyDrive/Final/book-recommender-model-v3"  # Update with your saved model path
tokenizer = T5Tokenizer.from_pretrained(model_path) # this is the tokenizer that matches my t5-base model
model = T5ForConditionalGeneration.from_pretrained(model_path) # This allows for text generation from my t5-base model
device = "cuda" if torch.cuda.is_available() else "cpu" # This tells the colab to use GPU if possible. If not, use CPU
model.to(device)

# Book Recommendation Function
def recommend_books(prompt):
    # Step 1: Embed the input and search FAISS
    query_embedding = embedding_model.encode([prompt], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, 5)  # Retrieve top 5 matches

    # Step 2: Retrieve books from dataset
    retrieved_books = dataset.iloc[indices[0]][["name", "summary"]]

    # Debugging: Ensure books are retrieved
    if retrieved_books.empty:
        return "‚ö†Ô∏è No books found. Try a different description!"

    summaries = retrieved_books["summary"].tolist()

    # Step 3: Generate a recommendation using the LLM
    input_text = f"Recommend a book based on these descriptions: {' '.join(summaries)}"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="max_length", max_length=256).to(device)

    with torch.no_grad():
        output = model.generate(**inputs, max_length=50)

    recommendation = tokenizer.decode(output[0], skip_special_tokens=True)

    # Step 4: Format the book list output
    book_list = "\n\n".join([f"üìñ **{row['name']}**\n{row['summary']}" for _, row in retrieved_books.iterrows()])

    return f"ü§ñ **AI Recommendation:** {recommendation}\n\nüîé **Top Matches:**\n\n{book_list}"

# Create a Gradio interface with a dedicated output box
interface = gr.Interface(
    fn=recommend_books,
    inputs=gr.Textbox(label="Enter a book description", placeholder="Describe the type of book you're looking for..."),
    outputs=gr.Textbox(label="Recommended Books", lines=10, interactive=False),  # Dedicated output box
    title="üìö AI-Powered Book Recommendation System",
    description="Enter a book description and get AI-generated recommendations based on similar books!",
)

# Launch the app
interface.launch()